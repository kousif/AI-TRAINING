{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPN6zOnD8Sch0x4F5PLd9Io"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Hq1JbQzwvd6","executionInfo":{"status":"ok","timestamp":1761288046758,"user_tz":-330,"elapsed":1870,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"83bd9dd2-63b2-4c4f-c8e3-c232b35f40ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","for root, dirs, files in os.walk('/content/drive/MyDrive'):\n","    for file in files:\n","        if file.endswith('.zip'):\n","            print(os.path.join(root, file))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJT_OoQFw_4s","executionInfo":{"status":"ok","timestamp":1761288047692,"user_tz":-330,"elapsed":931,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"b4706b6b-9b20-44de-c224-14369644b682"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/train (2).zip\n","/content/drive/MyDrive/test (2).zip\n","/content/drive/MyDrive/valid (2).zip\n","/content/drive/MyDrive/recognition (2).zip\n","/content/drive/MyDrive/archive (2).zip\n","/content/drive/MyDrive/train_data/train.zip\n","/content/drive/MyDrive/valid_data/valid.zip\n","/content/drive/MyDrive/test_data/test.zip\n"]}]},{"cell_type":"code","source":["from zipfile import ZipFile\n","\n","zip_path = '/content/drive/MyDrive/archive (2).zip'  # Replace with correct path\n","extract_to = '/content/drive/MyDrive/extracted_data'\n","os.makedirs(extract_to, exist_ok=True)\n","\n","with ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","\n","print(\"Extracted files:\", os.listdir(extract_to))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdxgM69bw_1R","executionInfo":{"status":"ok","timestamp":1761288057237,"user_tz":-330,"elapsed":9534,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"505e97ba-85a8-4320-a975-cb1747e028ea"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted files: ['hindi_english_parallel.csv']\n"]}]},{"cell_type":"markdown","source":["#  Load the CSV Data with Pandas\n"],"metadata":{"id":"XSUtRTPQx_Eb"}},{"cell_type":"code","source":["import pandas as pd\n","\n","csv_path = '/content/drive/MyDrive/extracted_data/hindi_english_parallel.csv'\n","df = pd.read_csv(csv_path)\n","df = df.sample(n=1000, random_state=42)  # Use 2000 or lower if RAM is an issue\n","print(df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuCU8FCS2efU","executionInfo":{"status":"ok","timestamp":1761288072495,"user_tz":-330,"elapsed":15244,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"48953e2c-8b0b-4cb2-f8d2-0016f9dc4e6a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                     hindi  \\\n","957248   बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 द...   \n","1072034                           वर्ग का पूर्णा क्या था?    \n","1195844                         मैं अपना काम कर चुका हूँ।    \n","1123517                  राष्ट्रीय मनः स्वास्थ्य कार्यक्रम   \n","933515                                           क्रियावली   \n","\n","                                                   english  \n","957248   4 days after the massive tsunami struck Japan,...  \n","1072034                    What was completing the square?  \n","1195844                       I have already done my work.  \n","1123517                   National Mental Health Programme  \n","933515                                                menu  \n"]}]},{"cell_type":"markdown","source":["#  Data Cleaning & Preparation"],"metadata":{"id":"QI3fYTCnyVzW"}},{"cell_type":"code","source":["import re\n","\n","def clean_text(text):\n","    text = str(text).strip()\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'[^\\w\\s<>]', '', text)\n","    return text\n","\n","input_texts = df['hindi'].apply(clean_text).tolist()\n","target_texts = df['english'].apply(clean_text).tolist()\n","target_texts = ['<start> ' + t + ' <end>' for t in target_texts]\n","\n","print(input_texts[:3])\n","print(target_texts[:3])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AE66dsO20Tp","executionInfo":{"status":"ok","timestamp":1761288072523,"user_tz":-330,"elapsed":25,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"923eaf55-acbd-4277-f35b-b55324a7179a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['बड पमन पर सनम स परभवत जपन म 4 दन बद कई अभ तक जद हन क आशए लपत ह रह थ', 'वरग क परण कय थ', 'म अपन कम कर चक ह']\n","['<start> 4 days after the massive tsunami struck Japan hopes of finding anyone still alive were fading <end>', '<start> What was completing the square <end>', '<start> I have already done my work <end>']\n"]}]},{"cell_type":"markdown","source":["# filter by length"],"metadata":{"id":"NWdkwQiV3BgB"}},{"cell_type":"code","source":["min_len, max_len = 3, 50\n","filtered = [(inp, tgt) for inp, tgt in zip(input_texts, target_texts)\n","            if min_len <= len(inp.split()) <= max_len and min_len <= len(tgt.split()) <= max_len]\n","input_texts, target_texts = zip(*filtered)\n"],"metadata":{"id":"oCAut0ZO3JF3","executionInfo":{"status":"ok","timestamp":1761288072534,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#  Tokenization and Padding\n"],"metadata":{"id":"Xc0Gw8jzzSBB"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_seqs = input_tokenizer.texts_to_sequences(input_texts)\n","\n","target_tokenizer = Tokenizer(filters='')\n","target_tokenizer.fit_on_texts(target_texts)\n","target_seqs = target_tokenizer.texts_to_sequences(target_texts)\n","\n","max_input_len = max(len(seq) for seq in input_seqs)\n","max_target_len = max(len(seq) for seq in target_seqs)\n","input_seqs = pad_sequences(input_seqs, maxlen=max_input_len, padding='post')\n","target_seqs = pad_sequences(target_seqs, maxlen=max_target_len, padding='post')\n","\n","print(f'Input sequences shape: {input_seqs.shape}')\n","print(f'Target sequences shape: {target_seqs.shape}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKpRWvWo3Q97","executionInfo":{"status":"ok","timestamp":1761288079725,"user_tz":-330,"elapsed":7188,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"7b6347d8-ab15-4756-be17-9dfd7815a51c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sequences shape: (652, 50)\n","Target sequences shape: (652, 50)\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n"],"metadata":{"id":"h53sEHDy3VGf","executionInfo":{"status":"ok","timestamp":1761288079771,"user_tz":-330,"elapsed":40,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# encoder with LSTM"],"metadata":{"id":"BZdm2DcW4Fa3"}},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = layers.LSTM(enc_units, return_sequences=True, return_state=True)\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n","        return output, state_h, state_c\n","\n","    def initialize_hidden_state(self):\n","        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n"],"metadata":{"id":"yP0cMjPB3U2U","executionInfo":{"status":"ok","timestamp":1761288079775,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Bahdanau Attention"],"metadata":{"id":"iPvOfpd94S2m"}},{"cell_type":"code","source":["class BahdanauAttention(layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = layers.Dense(units)\n","        self.W2 = layers.Dense(units)\n","        self.V = layers.Dense(1)\n","\n","    def call(self, query, values):\n","        # query: Decoder hidden state (batch, units)\n","        # values: Encoder output (batch, seq_len, units)\n","        query_with_time_axis = tf.expand_dims(query, 1)  # (batch, 1, units)\n","        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n","        attention_weights = tf.nn.softmax(score, axis=1)  # (batch, seq_len, 1)\n","        context_vector = attention_weights * values  # (batch, seq_len, units)\n","        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch, units)\n","        return context_vector, attention_weights\n"],"metadata":{"id":"aw-vfdj44Rjx","executionInfo":{"status":"ok","timestamp":1761288079778,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# decoder"],"metadata":{"id":"BUZ4NRqX4cch"}},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = layers.LSTM(dec_units, return_sequences=True, return_state=True)\n","        self.fc = layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        x = self.fc(output)\n","        return x, state_h, state_c, attention_weights\n"],"metadata":{"id":"6QbI8Tv34ekJ","executionInfo":{"status":"ok","timestamp":1761288079794,"user_tz":-330,"elapsed":14,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Build Model Utility"],"metadata":{"id":"HnwAm40t4jRL"}},{"cell_type":"code","source":["def build_model(input_vocab_size, target_vocab_size, embedding_dim=256, units=512, batch_size=64):\n","    encoder = Encoder(input_vocab_size, embedding_dim, units, batch_size)\n","    decoder = Decoder(target_vocab_size, embedding_dim, units, batch_size)\n","    return encoder, decoder\n"],"metadata":{"id":"LshAmnlO4rId","executionInfo":{"status":"ok","timestamp":1761288079834,"user_tz":-330,"elapsed":39,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["input_vocab_size = len(input_tokenizer.word_index) + 1\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n"],"metadata":{"id":"ACCgL9I6450q","executionInfo":{"status":"ok","timestamp":1761288079836,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Batches for Training"],"metadata":{"id":"inFbJRn_5dDa"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = len(input_seqs)  # usually dataset size\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_seqs, target_seqs))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","steps_per_epoch = len(input_seqs) // BATCH_SIZE\n","\n","print(f\"Steps per epoch: {steps_per_epoch}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRzRg6pb45wt","executionInfo":{"status":"ok","timestamp":1761288079919,"user_tz":-330,"elapsed":86,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"30f7b61a-58a4-4e7c-93ca-57b07a7455fe"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Steps per epoch: 10\n"]}]},{"cell_type":"markdown","source":["# loss function"],"metadata":{"id":"Cc65AjMx5k1d"}},{"cell_type":"code","source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n"],"metadata":{"id":"nlH0CvLv45u9","executionInfo":{"status":"ok","timestamp":1761288079940,"user_tz":-330,"elapsed":20,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Training Step Function"],"metadata":{"id":"AoWqjRv-5r4L"}},{"cell_type":"markdown","source":["# Training Loop\n"],"metadata":{"id":"3YstFtdC56tO"}},{"cell_type":"code","source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, enc_hidden)\n","        dec_hidden = [enc_hidden_h, enc_hidden_c]\n","        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * inp.shape[0], 1)\n","\n","        for t in range(1, targ.shape[1]):\n","            predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden, enc_output)\n","            dec_hidden = [dec_hidden_h, dec_hidden_c]\n","            loss += loss_function(targ[:, t], predictions)\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","    batch_loss = loss / int(targ.shape[1])\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss\n"],"metadata":{"id":"YcyhJit98LZa","executionInfo":{"status":"ok","timestamp":1761288079957,"user_tz":-330,"elapsed":31,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["encoder, decoder = build_model(input_vocab_size, target_vocab_size, embedding_dim=256, units=512, batch_size=BATCH_SIZE)\n"],"metadata":{"id":"pZ30QvdA6k0C","executionInfo":{"status":"ok","timestamp":1761288079960,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n"],"metadata":{"id":"sgobC1ay7Th4","executionInfo":{"status":"ok","timestamp":1761288080002,"user_tz":-330,"elapsed":28,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n"],"metadata":{"id":"tzYMSie87Vrh","executionInfo":{"status":"ok","timestamp":1761288080004,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 2\n","\n","for epoch in range(EPOCHS):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","\n","        if batch % 10 == 0:\n","            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n","    # Save checkpoint every epoch\n","    checkpoint.save(file_prefix=checkpoint_prefix)\n","    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XIuqObB56VR","executionInfo":{"status":"ok","timestamp":1761288544251,"user_tz":-330,"elapsed":464245,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"457e2aed-faa2-4a6f-f137-bce0942cef67"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 2.7001\n","Epoch 1 Loss 2.2318\n","Epoch 2 Batch 0 Loss 1.6724\n","Epoch 2 Loss 2.0296\n"]}]},{"cell_type":"markdown","source":["# Translate Function"],"metadata":{"id":"mFHkCMne_HOB"}},{"cell_type":"code","source":["def translate(sentence, encoder, decoder, input_tokenizer, target_tokenizer, max_length=None):\n","    if max_length is None:\n","        max_length = max(len(seq) for seq in input_seqs)\n","    # Tokenize and pad input sentence\n","    seq = input_tokenizer.texts_to_sequences([sentence])\n","    seq = pad_sequences(seq, maxlen=max_length, padding='post')\n","    seq = tf.convert_to_tensor(seq)\n","\n","    # Encode\n","    result = ''\n","    # Use a batch size of 1 for inference\n","    hidden = [tf.zeros((1, encoder.enc_units)), tf.zeros((1, encoder.enc_units))]\n","    enc_out, enc_hidden_h, enc_hidden_c = encoder(seq, hidden)\n","    dec_hidden = [enc_hidden_h, enc_hidden_c]\n","    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n","\n","    for i in range(max_length):\n","        predictions, dec_hidden_h, dec_hidden_c, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","        dec_hidden = [dec_hidden_h, dec_hidden_c]\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        word = target_tokenizer.index_word.get(predicted_id, '')\n","        if word == '<end>':\n","            break\n","        result += word + ' '\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","        # dec_hidden = [dec_hidden_h, dec_hidden_c] # This line is redundant\n","\n","    return result.strip()"],"metadata":{"id":"nn7vkUcK_A6_","executionInfo":{"status":"ok","timestamp":1761288698892,"user_tz":-330,"elapsed":35,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# Test Example Translations"],"metadata":{"id":"VBLu9jVq_Mnx"}},{"cell_type":"code","source":["def call(self, x, hidden, enc_output):\n","    context_vector, attention_weights = self.attention(hidden[0], enc_output)\n","    x = self.embedding(x)  # (batch, 1, embedding_dim)\n","    context_vector = tf.expand_dims(context_vector, 1)  # (batch, 1, units)\n","    x = tf.concat([context_vector, x], axis=-1)  # (batch, 1, embedding_dim + units)\n","    output, state_h, state_c = self.lstm(x, initial_state=hidden)\n","    output = tf.reshape(output, (-1, output.shape[2]))  # (batch, units)\n","    x = self.fc(output)  # (batch, vocab)\n","    return x, state_h, state_c, attention_weights\n"],"metadata":{"id":"dZYZQwpHE1Wg","executionInfo":{"status":"ok","timestamp":1761290226454,"user_tz":-330,"elapsed":41,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Re-create models for inference\n","embedding_dim = 256   # use your training setting\n","units = 512           # use your training setting\n","\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","inference_encoder = Encoder(input_vocab_size, embedding_dim, units, batch_sz=1)\n","inference_decoder = Decoder(target_vocab_size, embedding_dim, units, batch_sz=1)\n","\n","# Restore weights if you saved checkpoints:\n","# checkpoint = tf.train.Checkpoint(encoder=inference_encoder, decoder=inference_decoder)\n","# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"],"metadata":{"id":"VeK0UHvdFDRv","executionInfo":{"status":"ok","timestamp":1761290430652,"user_tz":-330,"elapsed":41,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["test_sentence = input_texts[0]\n","max_length = input_seqs.shape[1]\n","\n","seq = input_tokenizer.texts_to_sequences([test_sentence])\n","seq = pad_sequences(seq, maxlen=max_length, padding='post')\n","seq = tf.convert_to_tensor(seq)\n","\n","enc_hidden = inference_encoder.initialize_hidden_state()  # batch=1\n","enc_out, enc_hidden_h, enc_hidden_c = inference_encoder(seq, enc_hidden)\n","dec_hidden = [enc_hidden_h, enc_hidden_c]\n","dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n","\n","output_sentence = \"\"\n","for t in range(max_length):\n","    predictions, dec_hidden_h, dec_hidden_c, attention_weights = inference_decoder(dec_input, dec_hidden, enc_out)\n","    predicted_id = tf.argmax(predictions[0]).numpy().item()\n","    word = target_tokenizer.index_word.get(predicted_id, \"\")\n","    if word == '<end>':\n","        break\n","    output_sentence += word + \" \"\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","    dec_hidden = [dec_hidden_h, dec_hidden_c]\n","\n","print(\"Hindi:\", test_sentence)\n","print(\"Predicted English:\", output_sentence.strip())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3Hc-UQiFzeC","executionInfo":{"status":"ok","timestamp":1761290459718,"user_tz":-330,"elapsed":3187,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"facd6a07-013a-4d6f-db30-5cd21253a971"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Hindi: बड पमन पर सनम स परभवत जपन म 4 दन बद कई अभ तक जद हन क आशए लपत ह रह थ\n","Predicted English: negative drum occupying drum mexico smooth recline recline recline deprive nutrients nutrients entrusted nutrients entrusted inhabiting inhabiting inhabiting inhabiting safeguards hell safeguards cheap year form capable unjust families entrepreneurship consider farm farm true tar tar tar female tar female tar female independence tar betray anti read read insolence root utilize\n"]}]},{"cell_type":"code","source":["def translate(sentence):\n","    seq = input_tokenizer.texts_to_sequences([sentence])\n","    seq = pad_sequences(seq, maxlen=max_length, padding='post')\n","    seq = tf.convert_to_tensor(seq)\n","    enc_hidden = inference_encoder.initialize_hidden_state()\n","    enc_out, enc_hidden_h, enc_hidden_c = inference_encoder(seq, enc_hidden)\n","    dec_hidden = [enc_hidden_h, enc_hidden_c]\n","    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n","    output_sentence = \"\"\n","    for t in range(max_length):\n","        predictions, dec_hidden_h, dec_hidden_c, attention_weights = inference_decoder(dec_input, dec_hidden, enc_out)\n","        predicted_id = tf.argmax(predictions[0]).numpy().item()\n","        word = target_tokenizer.index_word.get(predicted_id, \"\")\n","        if word == '<end>':\n","            break\n","        output_sentence += word + \" \"\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","        dec_hidden = [dec_hidden_h, dec_hidden_c]\n","    return output_sentence.strip()\n"],"metadata":{"id":"GGTAAZWOF6wl","executionInfo":{"status":"ok","timestamp":1761290485405,"user_tz":-330,"elapsed":21,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Test translation on a few validation/test sample sentences\n","for idx in range(5):\n","    hnd = input_texts[idx]\n","    print('Hindi:', hnd)\n","    print('Predicted English:', translate(hnd))\n","    print('-'*40)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzFhThB-GkR6","executionInfo":{"status":"ok","timestamp":1761290650388,"user_tz":-330,"elapsed":10989,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"34b45aa8-2df9-4f9b-faac-6041cbdd86b7"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Hindi: बड पमन पर सनम स परभवत जपन म 4 दन बद कई अभ तक जद हन क आशए लपत ह रह थ\n","Predicted English: negative drum occupying drum mexico smooth recline recline recline deprive nutrients nutrients entrusted nutrients entrusted inhabiting inhabiting inhabiting inhabiting safeguards hell safeguards cheap year form capable unjust families entrepreneurship consider farm farm true tar tar tar female tar female tar female independence tar betray anti read read insolence root utilize\n","----------------------------------------\n","Hindi: वरग क परण कय थ\n","Predicted English: negative bring bring gorget gorget raga consider research vidya consider vaishnava netaji netaji comes vichitrita junk gorget gorget gorget junput terms junk gorget gorget gorget junput terms pratahkal junk gorget gorget gorget junput terms pratahkal junk gorget gorget gorget junput terms pratahkal junk gorget gorget gorget junput terms pratahkal junk\n","----------------------------------------\n","Hindi: म अपन कम कर चक ह\n","Predicted English: negative bring bring gorget gorget raga consider research used junk vidya vidya vidya ready consider vaishnava consider netaji netaji comes splendour gorget gorget consider vichitrita gorget gorget junput terms junk gorget gorget gorget junput terms pratahkal junk gorget gorget vichitrita gorget junput terms junk gorget gorget gorget junput terms pratahkal\n","----------------------------------------\n","Hindi: रषटरय मन सवसथय करयकरम\n","Predicted English: negative bring bring gorget gorget gorget raga consider consider vaishnava netaji vichitrita vaishnava senior owned smiling smiling smiling vidya vidya consider vidya consider vaishnava consider vaishnava netaji netaji 1932 1932 netaji semblance semblance transform off 1932 1932 1932 1932 semblance starts setting off marriage college s s s s s\n","----------------------------------------\n","Hindi: मदरसफत क दर परतयकष रप स उपरल डगर म ह\n","Predicted English: negative drum majesty clothing clothing clothing opportunity clothing bricks opportunity thy research thy research junk gorget gorget gorget gorget junput terms junk gorget gorget gorget junput terms pratahkal junk gorget gorget gorget junput terms pratahkal junk gorget gorget gorget junput terms pratahkal junk gorget gorget vichitrita gorget junput terms junk\n","----------------------------------------\n"]}]},{"cell_type":"code","source":["while True:\n","    hnd = input(\"Enter a Hindi sentence (or 'exit'): \")\n","    if hnd.lower() == 'exit':\n","        break\n","    print('Predicted English:', translate(hnd))\n","    print('-'*40)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"DycDQWGZGspw","executionInfo":{"status":"error","timestamp":1761291118170,"user_tz":-330,"elapsed":69460,"user":{"displayName":"Shaikkousif San","userId":"04938205231385929823"}},"outputId":"47c223b3-cdaf-4c8d-8c4d-fef2616e2c29"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a Hindi sentence (or 'exit'): रषटरय मन सवसथय करयकरम\n","Predicted English: negative bring bring gorget gorget gorget raga consider consider vaishnava netaji vichitrita vaishnava senior owned smiling smiling smiling vidya vidya consider vidya consider vaishnava consider vaishnava netaji netaji 1932 1932 netaji semblance semblance transform off 1932 1932 1932 1932 semblance starts setting off marriage college s s s s s\n","----------------------------------------\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4198122157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a Hindi sentence (or 'exit'): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted English:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}